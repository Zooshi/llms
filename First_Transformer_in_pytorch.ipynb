{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3t8xInnMHXULa7/TiDsuB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zooshi/llms/blob/main/First_Transformer_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n2pX7InIdMEv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Tokenizer, GPTSw3Tokenizer\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model,nhead,num_layers):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,d_model)\n",
        "    self.transformer = nn.Transformer(d_model = d_model, nhead=nhead,num_encoder_layers=num_layers)\n",
        "    self.fc = nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.embedding(x)\n",
        "    out = self.transformer(x,x)\n",
        "    out = self.fc(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "DjcTIb5Ed8Q-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000  # Example vocabulary size\n",
        "d_model = 256      # Model's hidden dimension\n",
        "nhead = 8          # Number of attention heads\n",
        "num_layers = 6     # Number of transformer layers\n"
      ],
      "metadata": {
        "id": "v06tFKIZd51m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(vocab_size,d_model,nhead,num_layers)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0wXREkFe2Sm",
        "outputId": "1e96378e-1182-477b-89e6-5ef456d344e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (embedding): Embedding(10000, 256)\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=256, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "AbJEbnKLe_5O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt,max_length=50, temperature = 0.1):\n",
        "  with torch.no_grad():\n",
        "    tokenized = torch.tensor([tokenizer.encode(prompt)])\n",
        "    #print(tokenized)\n",
        "    for _ in range(max_length):\n",
        "      logits = model(tokenized)\n",
        "      #print(logits)\n",
        "      logits = logits[:,-1,:]\n",
        "      next_token  =torch.multinomial(F.softmax(logits,dim=-1),num_samples=1)\n",
        "      tokenized = torch.cat((tokenized,next_token),dim=1)\n",
        "      print\n",
        "    for i in tokenized:\n",
        "      if i>50000:\n",
        "        i = 50000\n",
        "    generated_text = tokenizer.decode(tokenized[0].tolist(),skip_special_tokens= True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "Y32AmayKfHgN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "promptinput = \"Whey protein is\""
      ],
      "metadata": {
        "id": "bJcQi9z6gJ9F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_output = generate_text(promptinput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "i3cq-dyygSw-",
        "outputId": "0eb89c9d-e541-4a6b-b674-e95ca6dfa62c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-7bb01fde086f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromptinput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-07dbc1c02acb>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(prompt, max_length, temperature)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "class GPT2Simple(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
        "        super(GPT2Simple, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output = self.transformer(x, x)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 100000  # Example vocabulary size\n",
        "d_model = 256      # Model's hidden dimension\n",
        "nhead = 8          # Number of attention heads\n",
        "num_layers = 6     # Number of transformer layers\n",
        "\n",
        "# Create the model\n",
        "model = GPT2Simple(vocab_size, d_model, nhead, num_layers)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define a function to generate text based on a prompt\n",
        "def generate_text(prompt, max_length=50, temperature=1.0):\n",
        "    with torch.no_grad():\n",
        "        tokenized_prompt = torch.tensor([tokenizer.encode(prompt)])\n",
        "        tokenized_prompt = tokenized_prompt.to(device)\n",
        "        output = tokenized_prompt\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            logits = model(output)  # Get logits for the next token\n",
        "            logits = logits[:, -1, :] / temperature  # Apply temperature\n",
        "            next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "            if next_token == None:\n",
        "              next_token = 999\n",
        "            output = torch.cat((output, next_token), dim=1)\n",
        "            #print(output[0].tolist())\n",
        "            output = torch.clip(output,0,50000)\n",
        "        generated_text = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "# Provide a prototype or prompt\n",
        "prototype = \"Whey Protein is\"\n",
        "\n",
        "# Generate text using the prototype\n",
        "generated_output = generate_text(prototype, max_length=10, temperature=0.1)\n",
        "\n",
        "# Print the generated output\n",
        "print(\"Generated Output:\", generated_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf0o3bB3hGM1",
        "outputId": "0a2b3227-088b-490b-871e-c39c0312aa14"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Output: Whey Protein is Bridges gridsomersUnix ApocalypseResp Toronto gridsomers welcomed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqGH_PMgnZhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([10842, 88, 31702, 318, 43305, 10889, 50023, 29695, 19378, 29695])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CpWONRN2kMbW",
        "outputId": "82b4a9d9-29cf-4789-c11d-62dfaceba9ad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Whey Protein is Shiv Sure cringe parody positioned parody'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"How are you\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpVL-XqvlSRQ",
        "outputId": "65fecc70-aac4-43df-caa5-1b701d8fcb2d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2437, 389, 345]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le_91cMjlVg2",
        "outputId": "cf409578-78cb-40df-e9b0-19733c8284c1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex = torch.tensor([10842, 88, 31702, 318, 43305, 10889, 50023, 77561, 69698, 29695, 19378, 90220, 88925, 29695])\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ54YgvOmrPm",
        "outputId": "31e1ff34-3801-4e24-d9db-fcc1eed15314"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10842,    88, 31702,   318, 43305, 10889, 50023, 77561, 69698, 29695,\n",
              "        19378, 90220, 88925, 29695])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.clip(ex,0,50000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVch8KIdmvaG",
        "outputId": "fc0e29b7-e9db-4f92-da9f-450cf6a4089a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10842,    88, 31702,   318, 43305, 10889, 50000, 50000, 50000, 29695,\n",
              "        19378, 50000, 50000, 29695])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}